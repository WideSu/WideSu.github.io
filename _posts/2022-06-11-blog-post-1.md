---
title: 'Speech Enhancement Network'
date: 2022-06-11
permalink: /posts/2022/06/blog-post-1/
tags:
  - Speech Enhancement Network
---

This week, I want to share two papers about speech enhancement network.
- [PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network](https://arxiv.org/pdf/1911.04697.pdf)
- [UltraSE: Single-Channel Speech Enhancement Using Ultrasound](https://dl.acm.org/doi/pdf/10.1145/3447993.3448626)

# What's speech seperation and enhancement(SSE)?
Speech enhancement is used to extract audio from noices. 

<p align="center">
  <img width="568" alt="image" src="https://user-images.githubusercontent.com/44923423/178217820-7d8aa9bc-5a8c-49e0-ba25-563b3883d521.png">
</p>

# How do people solve SSE task?

In the past, we use **classical solution** which needs prior knowledge(i.e., per-speaker feature engineering)[\[1\]](https://arxiv.org/pdf/1810.04826.pdf) or directional microphone arrays[\[2\]](https://www.merl.com/publications/docs/TR2016-072.pdf) to solve it, in rencent years, **deep learning techniques** have proliferated and significantly advanced the field, enabling single-microphone speaker-independent SSE [\[3\]](https://sci-hub.wf/10.1109/TASLP.2018.2842159)

<p align="center">
  <img width="568" alt="image" src="https://user-images.githubusercontent.com/44923423/178314850-e3ef7ea1-9071-4eb1-bfb3-12cb9bc0e794.png">
</p>

- [1] [Quan Wang, Hannah Muckenhirn, Kevin Wilson, Prashant Sridhar, Zelin Wu, John Hershey, Rif A Saurous, Ron J Weiss, Ye Jia, and Ignacio Lopez Moreno. Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking. In Proceedings of Interspeech, 2019.](https://arxiv.org/pdf/1810.04826.pdf)
- [2] [Hakan Erdogan, John R Hershey, Shinji Watanabe, Michael I Mandel, and Jonathan Le Roux. Improved mvdr beamforming using single-channel mask prediction networks. In Proceedings of Interspeech, 2016.](https://www.merl.com/publications/docs/TR2016-072.pdf)
- [3] [DeLiang Wang and Jitong Chen. Supervised speech separation based on deep learning: An overview. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2018.](https://sci-hub.wf/10.1109/TASLP.2018.2842159)

## SOTA of SSE task
**State-of-the-art solutions** have demonstrated around 10 dB improvement in average audio quality, in separating a mixture of 2 clean speeches [\[4\]](https://arxiv.org/pdf/1809.07454.pdf)

<p align="center">
<img width="587" alt="image" src="https://user-images.githubusercontent.com/44923423/178316073-c8554f54-1cfe-4d8b-8161-2a28548cb7a5.png">
</p>

- [4] [Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 2019.](https://arxiv.org/pdf/1809.07454.pdf)

## Current challenges

scenario of more than 2 speakers mixed with background noise received little attention. preliminary test [\[6\]](https://arxiv.org/pdf/1907.01160.pdf) revealed that existing deep learning models often underperform in such cases, because the unstructured background noise compromises their ability to identify separable structures in the speech streams. In addition, existing audio-only approaches cannot solve the label permutation problem, i.e., associating the model outputs to the desired speaker. Audio-visual algorithms [6] leverage video recordings of the speakers’ faces to simultaneously solve the SSE and permutation problems. However, the need for a camera at specific view angle and under amenable lighting condition limits their practical usability. [\[7\]](https://arxiv.org/pdf/1907.04975.pdf)

- [5] [Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. WHAM!: Extending Speech Separation to Noisy Environments. CoRR, abs/1907.01160, 2019.](https://arxiv.org/pdf/1907.01160.pdf)
- [6] [Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation.In Proceedings of ACM SIGGRAPH, 2018.](https://arxiv.org/pdf/1804.03619.pdf)
- [7] [Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. My lips are concealed: Audio-visual speech enhancement through obstructions. In Proceedings of Interspeech, 2019.](https://arxiv.org/pdf/1907.04975.pdf)

# Paper1. UltraSE: Single-Channel Speech Enhancement Using Ultrasound

## 1.0 Overview of UltraSE

In this paper, we propose to utilize ultrasound sensing as a complementary modality to separate the desired speaker voice from noises and interferences. Our method, called UltraSE, is applicable to commodity mobile devices (e.g., smartphones) equipped with a single microphone and loudspeaker. Figure 1 illustrates our basic idea. During the voice recording, UltraSE continuously emits an inaudible ultrasound wave, which is modulated by the speaker’s articulatory gestures (lip movement in particular) close to the smartphone. The signals recorded by the microphone thus contain both the audible sounds and inaudible reflections. As illustrated in Figure 1, whereas the audible sounds (“Green”) mix the targeted clean speech (“Black”) and other interferences plus background noise (“Blue”), the inaudible reflections (“Orange”) only capture the targeted user’s articulatory gesture motion which is correlated with the clean speech.
UltraSE employs a DNN framework to capture such correlation and denoise the audible sounds.

<p align="center">
  <img width="568" alt="image" src="https://user-images.githubusercontent.com/44923423/178215126-a39068f8-8c78-454d-aced-5ae6f09da373.png">
</p>

UltraSE faces 3 core design challenges.

### i) How to characterize the articulatory gestures by ultrasound despite interference? 
It is challenging to capture the fine-grained articulatory gestures since they are fast (−80 ∼ 80 cm/s) and subtle (< 5 cm displacement). Moreover, mutual interference exists between the speech and ultrasound due to harmonics and hardware artifacts. To address the challenge, we fully exploit the advantages of ultrasound, i.e., high sampling rate and perfect alignment with the clean speech in the time domain. We design the transmitted ultrasonic waveform to capture the short-term high-resolution Doppler spectrogram, and apply a onetime transmission volume calibration to reduce the cross-modality interference.

### ii) How to design a DNN model to fuse the two modalities and represent their correlation? 
Since the physical feature characteristics of the two modalities are different, we design a two-stream DNN architecture to process each and a self-attention mechanism to fuse them. Further, no existing method has addressed the cross-modal noise reduction problem which is fundamental to UltraSE, i.e., using one modality (ultrasound) to reconstruct another modality (speech) which is polluted by noise/interference. We thus propose a conditional GAN (cGAN) based training model, with a novel cross-modal similarity measurement network, to enable this capability.

### (iii) How to improve both intelligibility and quality for the enhanced speech? 
It is known that the amplitude of time-frequency (T-F) spectrogram is critical for speech intelligibility, whereas the phase determines the speech quality [8]. We thus expand UltraSE into a two-stage multi-domain DNN architecture, which prioritizes the optimization of intelligibility in the T-F domain, and then reconstructs phase in the T domain to improve speech quality. We place the multi-modal fusion network inside the T-F domain, based on the empirical observation that the articulatory gestures are more related to the speech intelligibility.

## 1.1 Contributions

UltraSE represents the **first audio-only method** to bring the SSE performance **close to multi-channel solutions**, while overcoming the **label permutation issue**. Through the UltraSE design, we make the following technical contributions: 
- (1) We design a **multi-modal multi-domain DNN framework** for **single-channel speech enhancement** which fuses the ultrasound and speech features, and simultaneously improves speech intelligibility and quality.
- (2) We design a **cGAN-based cross-modal training model** which effectively **captures the correlation between ultrasound and speech for multi-modal denoising**.
- (3) We collect a new speech **dataset—UltraSpeech**, and verify UltraSE’s performance in comparison with state-of-the-art solutions.

### Contribution1: multi-modal multi-domain DNN framework

### Contribution2: cGAN-based cross-modal training model


## 1.2 Things we can improve from it

## 1.3 Related Work
#### 1.3.1 Audio-only Speech Enhancement
##### T-F domain methods
Time-Frequency (T-F) domain methods aim to learn a spectrogram mask, i.e., a weighting matrix that can be multiplied with the noisy speech spectrogram to recover the desired clean speech [8]. The key problem lies in i) what type of mask should be used, and ii) how to use DNN to predict such a mask. Early stage solutions only estimate the amplitudes of a spectrogram by using real-valued Ideal Binary Mask (IBM) [9], Ideal Ratio Mask (IRM) [10] or Spectral Magnitude Mask (SMM) [11]. They then directly apply the original noisy phase on each T-F bin to generate the enhanced speech. Although these amplitude masking methods benefit speech intelligibility, they suffer from poor speech perceptual quality due to the unavoidable phase error. Complex Ideal Ratio Mask (cIRM) [12] and Phase-Sensitive Mask (PSM) [13] are then proposed to incorporate phase information. Recently, PHASEN [8] and Ni et al. [15] found that the estimated cIRM tends to downgrade to IRM, since the T-F domain phase is close to white noise especially for low-amplitude T-F bins. Thus, they proposed two-stream [8] or two-stage [14] networks to take both the IRM and cIRM and derive a combined training loss. For the model design, most T-F domain methods deem the T-F spectrogram as an image, and design DNN/CNN-based models [12, 15] to minimize the MSE/MAE loss between the estimated mask and ground truth. PHASEN [8] and Ouyang et al. [16] observed that the fundamental frequencies and speech harmonics are separated afar, and the correlation cannot be fully captured by CNN. So they adopt dilated convolution and frequency-domain attention instead. Unlike the hand-crafted MSE/MAE loss function, Soni et al. [17] further used GAN to discriminate whether the enhanced results are clean or noisy.

- [8] [Dacheng Yin, Chong Luo, Zhiwei Xiong, and Wenjun Zeng. Phasen: A phaseand-harmonics-aware speech enhancement network. In Proceedings of AAAI, 2020.](https://sci-hub.wf/10.1609/aaai.v34i05.6489)
- [9] [Guoning Hu and DeLiang Wang. Monaural speech segregation based on pitch tracking and amplitude modulation. IEEE Transactions on Neural Networks, 2004.](https://sci-hub.wf/10.1109/tnn.2004.832812)
- [10] [Arun Narayanan and DeLiang Wang. Ideal ratio mask estimation using deep neural networks for robust speech recognition. In Proceedings of IEEE ICASSP, 2013.](https://web.cse.ohio-state.edu/~wang.77/papers/Narayanan-Wang2.icassp13.pdf)
- [11] [Yuxuan Wang, Arun Narayanan, and DeLiang Wang. On training targets for supervised speech separation. IEEE/ACM transactions on audio, speech, and language processing, 2014.](https://sci-hub.wf/10.1109/taslp.2014.2352935)
- [12] [Donald S Williamson, Yuxuan Wang, and DeLiang Wang. Complex ratio masking for monaural speech separation. IEEE/ACM transactions on audio, speech, and language processing, 2015.](https://sci-hub.wf/10.1109/taslp.2015.2512042)
- [13] [Hakan Erdogan, John R Hershey, Shinji Watanabe, and Jonathan Le Roux. Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks. In Proceedings of IEEE ICASSP, 2015.](https://sci-hub.wf/10.1109/icassp.2015.7178061)
- [14] [Zhaoheng Ni and Michael I Mandel. Mask-dependent phase estimation for monaural speaker separation. In Proceedings of IEEE ICASSP, 2020.](https://arxiv.org/pdf/1911.02746.pdf)
- [15] [Se Rim Park and Jinwon Lee. A fully convolutional neural network for speech enhancement. In Proceedings of Interspeech, 2017.](https://arxiv.org/pdf/1609.07132.pdf)
- [16] [Zhiheng Ouyang, Hongjiang Yu, Wei-Ping Zhu, and Benoit Champagne. A fully convolutional neural network for complex spectrogram processing in speech enhancement. In Proceedings of IEEE ICASSP, 2019.](https://sci-hub.wf/10.1109/ICASSP.2019.8683423)
- [17] [Meet H Soni, Neil Shah, and Hemant A Patil. Time-frequency masking-based speech enhancement using generative adversarial network. In Proceedings of IEEE ICASSP, 2018.](https://sci-hub.wf/10.1109/ICASSP.2018.8462068)

##### Time domain methods
T domain methods: Time (T) domain methods divert around the error-prone phase prediction problem by processing the waveform directly. For example, Rethage et al. [18] modified the WaveNet; TCNN [19] proposed an encoder-decoder architecture with an additional temporal convolutional net; SEGAN [20] utilized a GANbased network to generate the 1D waveform of clean speech. Yet the performance of such methods is not among the top tier, since the speech auditory patterns, such as proximity in time/frequency, harmonics, and common amplitude/frequency modulation, are more prominent on a T-F spectrogram [3].

- [18] [Dario Rethage, Jordi Pons, and Xavier Serra. A wavenet for speech denoising. In Proceedings of IEEE ICASSP, 2018.]https://sci-hub.wf/10.1109/ICASSP.2018.8462417()
- [19] [Ashutosh Pandey and DeLiang Wang. Tcnn: Temporal convolutional neural network for real-time speech enhancement in the time domain. In Proceedings of IEEE ICASSP, 2019.](https://web.cse.ohio-state.edu/~wang.77/papers/Pandey-Wang1.icassp19.pdf)
- [20] [Santiago Pascual, Antonio Bonafonte, and Joan Serra. Segan: Speech enhancement generative adversarial network. In Proceedings of IEEE ICASSP, 2018.](https://arxiv.org/pdf/1703.09452.pdf)

##### Multi-domain methods
In recent concurrent work TFTNet [21], a learnable decoder replaces the iSTFT in the T-F domain to realize a joint T-F and T domain model for speech enhancement. Unlike TFTNet, our key insight is that the speech intelligibility is much more important than speech quality for speech enhancement. We thus design a two-stage multi-domain DNN network to prioritize the optimization of speech intelligibility in the T-F domain, and then reconstruct phase in the T domain to improve the speech quality.

- [21] [Chuanxin Tang, Chong Luo, Zhiyuan Zhao, Wenxuan Xie, and Wenjun Zeng. Joint time-frequency and time domain learning for speech enhancement. In Proceedings of IJCAI, 2020.](https://dl.acm.org/doi/pdf/10.5555/3491440.3491968)
##### Speech source separation
#### 1.3.2 Multi-modal Speech Enhancement
To tackle the permutation issue, audio-visual (AV) methods use a video recording of the subject’s face as a hint for the audio [32, 33]. Specifically, Ephart et al. [6] trained a speaker-independent speech separation model based on a large set of YouTube videos [6].
Afourasl et al. [7] found that even partially occluded videos of lip motion can assist speech separation. Nonetheless, AV approaches bear many drawbacks. Besides microphone, they need an additional camera pointing to the subject’s face under good lighting conditions, which is inconvenient and even infeasible in many typical use cases.
Moreover, camera is unusable in many privacy-sensitive locations.
The idea of using ultrasound as a complementary modality to enhance speech has been explored by previous works [34, 35]. However, these works [34, 35] all require special ultrasonic hardware.
In comparison, UltraSE only needs the single audio channel on the smartphone and overcomes practical challenges such as mutual interference between modalities. Besides, they use traditional methods, i.e., non-negative matrix factorisation [35] and nonlinear regression [34], and only show the performance of speech enhancement on ambient noise rather than speech interference. UltraSE further pushes the limits of this idea by designing a multi-modal multi-domain DNN framework to achieve similar performance for speech separation and enhancement with the audio-visual methods.

## Paper2: PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network

## 2.0 Overview of PHASEN

In this paper, we propose a phaseand-harmonics-aware deep neural network (DNN), named PHASEN, for this task. Unlike previous methods which directly use a complex ideal ratio mask to supervise the DNN learning, we design a two-stream network, where amplitude stream and phase stream are dedicated to amplitude and phase prediction. We discover that the two streams should communicate with each other, and this is crucial to phase prediction.
In addition, we propose frequency transformation blocks to catch long-range correlations along the frequency axis. Visualization shows that the learned transformation matrix implicitly captures the harmonic correlation, which has been proven to be helpful for T-F spectrogram reconstruction. With these two innovations, PHASEN acquires the ability to handle detailed phase patterns and to utilize harmonic patterns, getting 1.76dB SDR improvement on AVSpeech + AudioSet dataset.
It also achieves significant gains over Google’s network on this dataset. On Voice Bank + DEMAND dataset, PHASEN outperforms previous methods by a large margin on four metrics.

<img width="639" alt="image" src="https://user-images.githubusercontent.com/44923423/182860339-9f3c6f0d-b6f8-47dd-9bad-486581681009.png">

### 2.1 Contribution

Designed a two-stream network that predict amplitude and phase respectively

- Found that two stream should **communicate** with each other, which is crucial for phase estimation 

- Proposed **frequency transformation blocks(FTB)** to catch long-range correlation along the frequency

<img width="189" alt="image" src="https://user-images.githubusercontent.com/44923423/183311775-72f9eac2-a4b6-4bb2-b5a8-b992c0fb55a8.png">
- Performance: 

		- AVRSpeech + AudioSet : 1.76dB SDR +
					Significant gain over Google’s network		

		- Voice Bank + DEMAND: Outperform previous methods

### 2.2 Things we can improve from

### 2.3 Related Work

#### T-F domain masking methods

- Amplitude estimation

  - IBM 2001
  - IRM 2006
  - SMM 2014

- [x] [Hu, G., and Wang, D. 2001. Speech segregation based on pitch tracking and amplitude modulation. In Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No. 01TH8575), 79–82. IEEE.](https://sci-hub.wf/10.1109/aspaa.2001.969547)
- [x] [Binary and ratio time-frequency masks for robust speech recognition](https://sci-hub.wf/10.1016/j.specom.2006.09.003)

- Phase estimation
  - PSM 2015
  - clRM 2016
  - Bi-LSTM 2018: SOTA

#### Time domain methods
- SEGAN 2017: GANs to predict waveflow directly
- Conv-TasNet 2019: learnable encoder-decoder
- TCNN 2019: non-liner encoder-decoder

#### Harmonics in Spectrogram

- Phase reconstruction using frequency and phase distortion feature: 2018




