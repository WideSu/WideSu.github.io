---
title: 'Speech Enhancement Network'
date: 2022-06-11
permalink: /posts/2022/06/blog-post-1/
tags:
  - Speech Enhancement Network
---

This week, I want to share two papers about speech enhancement network.
- 1. [PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network](https://arxiv.org/pdf/1911.04697.pdf)
- 2. [UltraSE: Single-Channel Speech Enhancement Using Ultrasound](https://dl.acm.org/doi/pdf/10.1145/3447993.3448626)

# 0. Background of speech seperation and enhancement(SSE)

## 0.1 Classical solutions
- Prior knowledge(per-speaker feature engineering)
- Directional microphone arrays
Speech enhancement is used to extract audio from noices. 
**Classical solutions** need to rely on prior knowledge (i.e., per-speaker feature engineering) [1] or directional microphone arrays [2] to isolate the desired source from ambient sounds.

<p align="center">
  <img width="568" alt="image" src="https://user-images.githubusercontent.com/44923423/178217820-7d8aa9bc-5a8c-49e0-ba25-563b3883d521.png">
</p>

[1] Quan Wang, Hannah Muckenhirn, Kevin Wilson, Prashant Sridhar, Zelin Wu, John Hershey, Rif A Saurous, Ron J Weiss, Ye Jia, and Ignacio Lopez Moreno. Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking. In Proceedings of Interspeech, 2019.
[2] Hakan Erdogan, John R Hershey, Shinji Watanabe, Michael I Mandel, and Jonathan Le Roux. Improved mvdr beamforming using single-channel mask prediction networks. In Proceedings of Interspeech, 2016. 

## 0.2 Deep learning techniques 
In the past several years, **deep learning techniques** have proliferated and significantly advanced the field, enabling single-microphone speaker-independent SSE [3]

[3] DeLiang Wang and Jitong Chen. Supervised speech separation based on deep learning: An overview. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2018.

## 0.3 SOTA
**State-ofthe-art solutions** have demonstrated around 10 dB improvement in average audio quality, in separating a mixture of 2 clean speeches [4]

[4] Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 2019.

## 0.4 Current challenges

scenario of more than 2 speakers mixed with background noise received little attention. preliminary test [6] revealed that existing deep learning models often underperform in such cases, because the unstructured background noise compromises their ability to identify separable structures in the speech streams. In addition, existing audio-only approaches cannot solve the label permutation problem, i.e., associating the model outputs to the desired speaker. Audio-visual algorithms [6] leverage video recordings of the speakers’ faces to simultaneously solve the SSE and permutation problems. However, the need for a camera at specific view angle and under amenable lighting condition limits their practical usability [7].

[5] Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. WHAM!: Extending Speech Separation to Noisy Environments. CoRR, abs/1907.01160, 2019.
[6] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation.
In Proceedings of ACM SIGGRAPH, 2018.
[7] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. My lips are concealed: Audio-visual speech enhancement through obstructions. In Proceedings of Interspeech, 2019.

# Paper1. UltraSE: Single-Channel Speech Enhancement Using Ultrasound

## 1.1 Contributions

### 1.1.1 Breifing

UltraSE represents the first audio-only method to bring the SSE performance close to multi-channel solutions, while overcoming the label permutation issue. Through the UltraSE design, we make the following technical contributions: 
• We design a multi-modal multi-domain DNN framework for single-channel speech enhancement which fuses the ultrasound and speech features, and simultaneously improves speech intelligibility and quality.
• We design a cGAN-based cross-modal training model which effectively captures the correlation between ultrasound and speech for multi-modal denoising.
• We collect a new speech dataset—UltraSpeech, and verify UltraSE’s performance in comparison with state-of-the-art solutions.

### 1.1.2 Detailed explaination of UltraSE

In this paper, we propose to utilize ultrasound sensing as a complementary modality to separate the desired speaker voice from noises and interferences. Our method, called UltraSE, is applicable to commodity mobile devices (e.g., smartphones) equipped with a single microphone and loudspeaker. Figure 1 illustrates our basic idea. During the voice recording, UltraSE continuously emits an inaudible ultrasound wave, which is modulated by the speaker’s articulatory gestures (lip movement in particular) close to the smartphone. The signals recorded by the microphone thus contain both the audible sounds and inaudible reflections. As illustrated in Figure 1, whereas the audible sounds (“Green”) mix the targeted clean speech (“Black”) and other interferences plus background noise (“Blue”), the inaudible reflections (“Orange”) only capture the targeted user’s articulatory gesture motion which is correlated with the clean speech.
UltraSE employs a DNN framework to capture such correlation and denoise the audible sounds.

<p align="center">
  <img width="568" alt="image" src="https://user-images.githubusercontent.com/44923423/178215126-a39068f8-8c78-454d-aced-5ae6f09da373.png">
</p>

### 1.1.3 Challenges UltraSE solved

UltraSE faces 3 core design challenges.

#### i) How to characterize the articulatory gestures by ultrasound despite interference? 
It is challenging to capture the fine-grained articulatory gestures since they are fast (−80 ∼ 80 cm/s) and subtle (< 5 cm displacement). Moreover, mutual interference exists between the speech and ultrasound due to harmonics and hardware artifacts. To address the challenge, we fully exploit the advantages of ultrasound, i.e., high sampling rate and perfect alignment with the clean speech in the time domain. We design the transmitted ultrasonic waveform to capture the short-term high-resolution Doppler spectrogram, and apply a onetime transmission volume calibration to reduce the cross-modality interference.

#### ii) How to design a DNN model to fuse the two modalities and represent their correlation? 
Since the physical feature characteristics of the two modalities are different, we design a two-stream DNN architecture to process each and a self-attention mechanism to fuse them. Further, no existing method has addressed the cross-modal noise reduction problem which is fundamental to UltraSE, i.e., using one modality (ultrasound) to reconstruct another modality (speech) which is polluted by noise/interference. We thus propose a conditional GAN (cGAN) based training model, with a novel cross-modal similarity measurement network, to enable this capability.

#### (iii) How to improve both intelligibility and quality for the enhanced speech? 
It is known that the amplitude of time-frequency (T-F) spectrogram is critical for speech intelligibility, whereas the phase determines the speech quality [8]. We thus expand UltraSE into a two-stage multi-domain DNN architecture, which prioritizes the optimization of intelligibility in the T-F domain, and then reconstructs phase in the T domain to improve speech quality. We place the multi-modal fusion network inside the T-F domain, based on the empirical observation that the articulatory gestures are more related to the speech intelligibility.

## 1.2 Things we can improve from it

## 1.3 Related Work
#### 1.3.1 Audio-only Speech Enhancement
##### T-F domain methods
##### T domain methods
##### Multi-domain methods
##### Speech source separation
#### 1.3.2 Multi-modal Speech Enhancement
To tackle the permutation issue, audio-visual (AV) methods use a video recording of the subject’s face as a hint for the audio [32, 33]. Specifically, Ephart et al. [6] trained a speaker-independent speech separation model based on a large set of YouTube videos [6].
Afourasl et al. [7] found that even partially occluded videos of lip motion can assist speech separation. Nonetheless, AV approaches bear many drawbacks. Besides microphone, they need an additional camera pointing to the subject’s face under good lighting conditions, which is inconvenient and even infeasible in many typical use cases.
Moreover, camera is unusable in many privacy-sensitive locations.
The idea of using ultrasound as a complementary modality to enhance speech has been explored by previous works [34, 35]. However, these works [34, 35] all require special ultrasonic hardware.
In comparison, UltraSE only needs the single audio channel on the smartphone and overcomes practical challenges such as mutual interference between modalities. Besides, they use traditional methods, i.e., non-negative matrix factorisation [35] and nonlinear regression [34], and only show the performance of speech enhancement on ambient noise rather than speech interference. UltraSE further pushes the limits of this idea by designing a multi-modal multi-domain DNN framework to achieve similar performance for speech separation and enhancement with the audio-visual methods.
