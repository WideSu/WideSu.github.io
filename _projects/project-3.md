---
title: "Project 3-Airline delay prediction on Google Cloud Platform"
excerpt: "MLops on GCP<br/><img src='/images/proj3.png'>"
collection: projects
---

## Project Objective

Build a machine learning pipeline for CI/CD.

### Airline delay

As more countries come out of COVID lockdowns, travelling is expected to pick up. The global market size of the industry was 471.8 billion U.S. dollars in 2021. Countries are optimistic that the travel industry will reach pre-pandemic levels.  And it is estimated to increase by 4.5% in the coming year. Respondents of a late 2020 survey expressed that newly implemented measures have improved confidence during travel. It is believed that relaxation of travel restrictions will continue post-COVID era, combined with lower travel costs and a relative return to normalcy, will contribute to higher passenger numbers in the years to come. 

![image](https://user-images.githubusercontent.com/44923423/180154835-f37939df-fa35-4176-8666-48f5d42d0c9b.png)

While the airline industry has covered much from COVID, it is inevitable that flights will be subjected to disruption, resulting in delays and cancellations. Our project aims to model the arrival delay of airline flights to help passengers make their schedule more predictable. We ingested year 2015 records on-time performance dataset to predict the delay of an airline.


### Project statement
While the airline industry has covered much from COVID, it is inevitable that flights will be subjected to disruption, resulting in delays and cancellations. Our project aims to model the arrival delay of airline flights to help passengers make their schedule more predictable. We ingested year 2015 records on-time performance dataset to predict the delay of an airline.


## Dataset overview

Our dataset is from Bauru of Transportation Statistics (BTS), which has 107 columns. After reading the descriptions of the fields, we find that there are a few fields that appear relevant to the problem of training, predicting, or evaluating flight arrival delay. Table 1 presents the fields shortlisted by the team.

![image](https://user-images.githubusercontent.com/44923423/180155549-ab580e1f-35f7-43e4-a4f2-67dcc8e698ff.png)

### EDA

Before feature engineering, we firstly did some EDA to have a general understanding of our dataset. In order to get a sense of departure delay for each airline, we draw a bee colony diagram as shown in Fig 2. The figure makes a census of all the delays that were measured in January 2015. This representation gives a feeling on the dispersion of data and put in perspective the relative homogeneity that appeared in the second pie chart. Indeed, we see that mean delays are around 10 minutes, this low value is a consequence of the fact that most flights take off on time. However, we occasionally face large delays reaching over 10+ hours.

Fig 2: Departure delay for each airline 
![image](https://user-images.githubusercontent.com/44923423/180155723-31c7a24f-b9ca-43e2-a4f9-a06cba44e11e.png)

In addition, we are curious about whether the delay is due to departure or landing. A stacked bar chart (Fig 3) shows the blue bar for departure delay and the bar with hatch lines for arrival delay. We can see that delays at arrival are generally lower than at departure. This indicates that flights typically adjust their flight speed in order to reduce the delays at arrival.

Fig 3: Mean delay during departure vs arrival for each airline 
![image](https://user-images.githubusercontent.com/44923423/180156246-2ae68ad8-2e28-4af2-b256-950284cbd5dc.png)

### Proposed pipeline

Our pipeline consists of four parts: creating tabular dataset, AutoML training, model evaluation and validation, and model deployment as shown in Fig 4. The component definition specifies a base image for the component to use and specifies that the google-cloud-ai platform package should be installed. The custom pipeline component retrieves the regression model evaluation generated by the AutoML tabular training process, parses the evaluation data, and renders the RMSE and MAE for the model. It also uses given metrics threshold information and compares that to the evaluation results to determine whether the model is sufficiently accurate for deploy.

Fig 4: Pipeline overview
![image](https://user-images.githubusercontent.com/44923423/180156507-d38df66d-6f18-4530-ac54-926e18a6915d.png)

#### Data Ingestion
For data ingestion, we firstly gathered data from BTS website, then did feature engineering including deleting the columns with too many missing values and change them into proper data types. This is followed by selection of important features. Finally, the csv files are ingested into into BigQuery so we can build the AutoML dataset by querying data from BigQuery, as shown in Fig 5.

Fig 5: Pipeline overview
![image](https://user-images.githubusercontent.com/44923423/180156759-947db3b4-76d1-4767-a91a-9b18d97ba9ff.png)

### Model Training
We used an AutoML tabular training component to train our model which is defined from google_cloud_pipeline_components as shown in Fig 6. AutoML Tables is a supervised learning service. This means that you train a machine learning model with example data. AutoML Tables uses tabular (structured) data to train a machine learning model to make predictions on new data. One column from your dataset, called the target, is what the model will learn to predict. Some numbers of the other data columns are inputs (called features) that the model will learn patterns from. You can use the same input features to build multiple models just by changing the target. From the email marketing example, this means you could build two models with the same input features: One model could predict a customer's persona (a categorical target), and one could predict their monthly spending (a numerical target).

Fig6: Model training section in the pipeline
![image](https://user-images.githubusercontent.com/44923423/180156909-7675fcfa-f162-4c31-a9b8-44784d5c75dc.png)

Below is an evaluation of the advantages and disadvantages of using AutoML based on the team’s experience of using GCP’s AutoML solution.
Pros & Cons of AutoML vs Custom Training
Pros
- AutoML solutions can use only a small amount of data for training and still obtain good results. This is because most AutoML solutions can rely on transfer learning.
- Good for problems which are generalisable to common problems.
- No need to build a machine learning model from scratch
- Big Tech AutoML vendors are more likely to incorporate state of art technology, reducing the burden of keep up with latest AI development.
Cons
- Not always applicable to niche use cases 
- Architecture of model in AutoML is typically unknown. Hence lesser insights can be derived as compared to doing Machine Learning by hand.
- High running cost/hosting expected when scaling up

### Conditional deployment
To avoid data shift and under-performance, we use conditional deployment (fFig 7) before deploying the model. To be specific, we will only deploy the model when the selected metrics are above a threshold. For the regression model and will use MAE and MSE as the thresholds to decide whether to use previous module or the new one.
Fig7: Conditional Deployment in the pipeline
![image](https://user-images.githubusercontent.com/44923423/180157216-ea0cfda2-a8e6-405d-a62c-a5b81843a9b6.png)

### CICD and Unit Test

The motivation for CI/CD is fairly straightforward. If you’re a developer, version control—whether it’s git branches or another system—is your source of truth. At the same time, code for Cloud Functions has to be tested and then redeployed manually. This presents no shortage of potential issues.
CI/CD systems automate this process, letting you automatically mirror any changes in version control to GCP deployments. CI/CD systems detect code changes using hooks in version control systems that are triggered whenever new code versions are received. These systems can also invoke language-specific command-line functions to run your tests, followed by a call to gcloud to automatically deploy any code changes to Cloud Functions.
The CI/CD consists of following procedures:
- 1)Preparation
Enable resource manager, GKE, cloud source repositories, cloud build, container registry
- 2)Creating GKE application
- 3)Automating deployments for git branches
- 4)Automating deployments for git main branch
- 5)Automating deployments for git tags
- 6)Clean up

The CI/CD routine is defined in the pipeline-deployment.yaml file, and consists of the following steps:
- 1)Clone the repository to the build environment
- 2)Run unit tests.
- 3)Run a local e2e test of the pipeline.
- 4)Build the ML container image for pipeline steps.
- 5)Compile the pipeline.
- 6)Upload the pipeline to Cloud Storage.

For unit tests, we tested three components, first is unit tests for data ingestion and validation. Second is model test which tests the model generated by AutoML. Third is model deployment test which tests the model uploaded by sending requests to the endpoint.

### Cloud Services
In our project, we use 3 cloud services, big query, cloud build, vertex AI. We use BigQuery to store the view of data, Cloud Build to get trigger from cloud commit auto-compile and build the docker image and push to the repository, and autoML to train a model and deploy it to end point.

Fig8: Cloud Services we use in this project
![image](https://user-images.githubusercontent.com/44923423/180157466-1ad596f5-14c3-4c23-9fb4-963e94108e64.png)

#### End Product

By using the endpoint of our trained model, we build a website application which enables user to key in their desired fly date and time and get to know the delay for each airline at this time as shown in Fig 8. For example, a tourist wants to fly from John F Kennedy International Airport in New York to San Francisco International Airport on July 2nd in the morning. He or she can select the specifics of the trip and use our model to predict its delay.

Fig 9: Index of our website where users can select origin, destination, airline and datetime to predict its delay
![image](https://user-images.githubusercontent.com/44923423/180157686-69b83865-7332-4e53-ad3f-6755f344309e.png)

After submitting, users can check the delay of historical delay for each airline and the predicted probability of delay by our model as shown in Fig 9. As we can see, the probability of delay is around 21%. And among all the airlines, Delta Airline has the least delay. Thus, the user can choose Delta Airline to avoid delay.
![image](https://user-images.githubusercontent.com/44923423/180157825-6d180d34-45bb-4cf1-bad4-1fb1a5aaca6b.png)

## Challenges and future work
Machine learning engineering is a tough task and not many resources on the Internet as machine learning does. When implementing the pipeline, we got many errors, and some errors are quite strange such like edge error, and we spend much time spend time debugging it.
Hyperparameter tuning requires multiple runs of the training application within limits of chosen hyperparameters. Vertex AI uses Bayesian optimisation for hypermeter tuning. Vertex can keep track of the results of each trial and the most effective configuration can then be applied for prediction. In AutoML, the stages in yellow are managed automatically. 

![image](https://user-images.githubusercontent.com/44923423/180158266-0daf2e82-ae40-4cec-ba9c-b04cf405ce0a.png)

**Model monitoring**
- Training-serving skew occurs when feature distribution in production deviates from the training data.
- Prediction drift occurs when feature distribution in production changes over time. This option is enabled when the original training data is not available. 
In the project, since we have the training data, we could enable the training-serving skew monitoring.